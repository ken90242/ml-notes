# 一、M的重要性

![](/assets/m74hfwh982port.png)

當M越小\(hypothesis的數量\)，其選到壞資料的機率就越小，然而選擇卻很少\($$E_{in}(g)\ small?$$\)

當M越大，其選擇變多，然而選到壞資料的機率卻增加了$$E_{out}(g)\ \ v.s.\ \ E_{in}(g)$$

M無論變大變小，都有其優缺點。

> 我們無法同時保證光是將M調到最大或最小
>
> **便可選到壞資料的機率是小的、讓訓練後的精準度是準的。**

# 二、如何調到剛剛好的M

因此M的大小不可太大、太小，因此如何取到剛好的M便是一門學問

hypothesis的數量將近無限多種，首先在避免選到壞資料便是個大問題



複習一下M是如何得出的:



![](/assets/impj843hf2ort.png)

